library(dplyr); library(beepr); library(parallel); library(tm)
library(SnowballC); library(NLP) # Load libraries
head(transaction)
colnames(transaction)
tran_sub <- subset(transaction, select = c('fiscal_year', 'award_id', 
  'recipient_location_state_code', 'total_obligation', 'awarding_toptier_agency_name'),
  transaction$fiscal_year > 2010 & transaction$fiscal_year < 2019 &
  transaction$total_obligation > 0 & 
  transaction$awarding_toptier_agency_name != '' &
  transaction$recipient_location_state_code %in% state.abb &
  transaction$award_id != '')

tran_sub$recipient_location_state_code <- 
  as.character(tran_sub$recipient_location_state_code)

# Combine duplicate award_id's
id_table <- table(tran_sub$award_id)
id_table_sub <- id_table[id_table > 1]
dupl_id <- names(id_table_sub)
tran_dupl <- tran_sub[tran_sub$award_id %in% dupl_id, ]
tran_not_dupl <- tran_sub[!tran_sub$award_id %in% dupl_id, ]
tran_dupl_unique <- unique_id_func(tran_dupl)
tran_comb <- rbind(tran_not_dupl, tran_dupl_unique)
beep()

# business categories, awarding toptier agency name
# by state and annually
tran_sub_year <- split(tran_sub, tran_sub$fiscal_year)

#### starting with tran_sub_year

# wnt to sort by state
year_to_state_desc_spend <- lapply(tran_sub_year, 
                                   function(x) year_to_state(yearly_df = x))
# each year now has two lists of length 50, state/spending

year_to_state <- function(yearly_df = tran_sub_year[[1]]) {
  # Returns two lists, where the 
  state_split <- split(yearly_df, yearly_df$recipient_location_state_code)
  state_desc <- lapply(state_split, function(x) x$awarding_toptier_agency_name)
  state_spend <- lapply(state_split, function(x) x$total_obligation)
  return(list(state_desc, state_spend))
}
# desc/spend per 50 states
# equal length lists with matching dataframes
awarding_desc <- lapply(year_to_state_desc_spend, function(x) 
  annual_desc_spend(year_list_element = x))

# by year ->
# take the desc/spend ->
annual_desc_spend <- function(year_list_element = year_to_state_desc_spend[[1]]) {
  # Takes the yearly list element and breaks down into list of 50 descriptions
  # and 50 spending amounts.
  year_desc <- year_list_element[[1]]
  year_spend <- year_list_element[[2]]
  
  # Go by state to find the weighted character vectors.
  # Creates a list of 50 states each with a list of an index and string.
  state_desc_word_filter <- lapply(year_desc, 
                                   function(x) word_filter(year_desc_element = x))
  
  # Add the spending to the list of 50.
  for(i in 1:length(state_desc_word_filter)) {
    state_desc_word_filter[[i]][length(state_desc_word_filter[[i]]) + 1] <- 
      year_spend[i]
  }
  
  # Sort through each 50 states in list and use the word filter.
  state_weighted_top25 <- lapply(state_desc_word_filter, function(x)
    state_list_to_char_vec(state_string_ind_spend = x))
  return(state_weighted_top25)
}

weighted_char_vectors <- function(price, character_vector) {
  # The weighted_char_vectors function will take in the price and character
  # vector which have previously been created. It will then split the 
  # character vector into the individual keywords of each expense, and then
  # assign the average price to each keyword. These keywords are then combined
  # so that keywords which are matching will also have their weighted prices
  # from across different expenses summed together.
  
  # Split the keywords from each exepense, and assign them to their average
  # prices from each character vector.
  keywords_list = strsplit(character_vector$keywords, ' ', fixed = TRUE)
  keywords_list_len = sapply(keywords_list, length)
  avg_price = character_vector$weighted_price / keywords_list_len
  weighted_words = Map(cbind, keywords_list, avg_price)
  weighted_words = do.call("rbind", weighted_words)
  
  # Turn the keywords into a dataframe, and then group the dataframe
  # by keyword to sum the total weighted average.
  weighted_df = as.data.frame(weighted_words, stringsAsFactors = FALSE)
  names(weighted_df) = c("keywords", "char_weights")
  weighted_df$char_weights = as.numeric(weighted_df$char_weights)
  sum_df = tapply(weighted_df$char_weights, weighted_df$keywords, 
                  function(x) sum(x, na.rm = TRUE))
  
  # Return the top 25 keywords, along with their weighted prices in terms
  # of their proportion.
  top_25 = head(sort(sum_df, decreasing = TRUE), n = 25)
  round(top_25 / sum(top_25), 3)
}

state_list_to_char_vec <- function(state_string_ind_spend = state_desc_word_filter[[1]]) {
  lead_trail <- state_string_ind_spend[[1]]
  rm_ind <- state_string_ind_spend[[2]]
  price <- state_string_ind_spend[[3]]
  
  character_vector <- data.frame(keywords = lead_trail[rm_ind], # df of keywords and prices
                                 weighted_price = price[rm_ind] /
                                   sapply(strsplit(lead_trail[rm_ind], " "), length),
                                 stringsAsFactors = FALSE)
  weighted_char_vectors(price = price[rm_ind], character_vector = character_vector)
}

word_filter <- function(year_desc_element = year_desc[[1]]) {
  removed_punc = gsub('[[:punct:]]', ' ', year_desc_element)
  lower_case = tolower(removed_punc)
  
  # Remove stopwords, create stem words
  stop_words1 = removeWords(lower_case, stopwords('en'))
  stop_words2 = removeWords(stop_words1, stopwords('SMART'))
  stem_words = stemDocument(stop_words2)
  
  # Deal with any white space and create removal index
  clean_whitespace = stripWhitespace(stem_words)
  remove_na = clean_whitespace[clean_whitespace != 'NA']
  rm_ind = which(remove_na != '')
  lead_trail = gsub("^\\s+|\\s+$", "", clean_whitespace[rm_ind])
  rm_ind = which(lead_trail != '')
  
  return(list(lead_trail, rm_ind))
}










